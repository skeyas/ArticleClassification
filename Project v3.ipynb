{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "import wikipedia\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCorpus(data, labels):\n",
    "    if type(labels) is list:\n",
    "        for i in range(len(labels)):\n",
    "            for j in range(3):\n",
    "                try:\n",
    "                    tokenizedText = word_tokenize(wikipedia.page(wikipedia.search(labels[i])[j]).content)\n",
    "                except:\n",
    "                    continue\n",
    "                for w in tokenizedText:\n",
    "                    w = w.lower()\n",
    "                    if w not in stop_words and w not in punctuation:\n",
    "                        if w.endswith(\"ies\"):\n",
    "                            w = w[:-3]\n",
    "                            w = w + \"y\"\n",
    "                        data.append(w)\n",
    "    elif type(labels) is str:\n",
    "        for i in range(4):\n",
    "            try:\n",
    "                tokenizedText = word_tokenize(wikipedia.page(wikipedia.search(labels)[i]).content)\n",
    "            except:\n",
    "                continue\n",
    "            for w in tokenizedText:\n",
    "                w = w.lower()\n",
    "                if w not in stop_words and w not in punctuation:\n",
    "                    if w.endswith(\"ies\"):\n",
    "                        w = w[:-3]\n",
    "                        w = w + \"y\"\n",
    "                    data.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@ezzatdemnati/web-scraping-news-data-rss-feeds-python-and-google-cloud-platform-7a0df2bafe44\n",
    "\n",
    "def getArticleText(url):\n",
    "    article = requests.get(url)\n",
    "    articles = BeautifulSoup(article.content, 'html.parser')\n",
    "    articles_body = articles.findAll('body')    \n",
    "    p_blocks = articles_body[0].findAll('p')\n",
    "    p_blocks_df=pd.DataFrame(columns=[\"element_name\",\"parent_hierarchy\",\"element_text\",\"element_text_Count\"])\n",
    "    for i in range(len(p_blocks)):\n",
    "        parents_list=[]\n",
    "        for parent in p_blocks[i].parents:\n",
    "            Parent_id = ''\n",
    "            try:\n",
    "                Parent_id = parent['id']\n",
    "            except:\n",
    "                pass\n",
    "            parents_list.append(parent.name + 'id: ' + Parent_id)\n",
    "        parent_element_list = ['' if (x == 'None' or x is None) else x for x in parents_list ]\n",
    "        parent_element_list.reverse()\n",
    "        parent_hierarchy = ' -> '.join(parent_element_list)\n",
    "        p_blocks_df=p_blocks_df.append({\"element_name\":p_blocks[i].name\n",
    "                                  ,\"parent_hierarchy\":parent_hierarchy\n",
    "                                  ,\"element_text\":p_blocks[i].text\n",
    "                                  ,\"element_text_Count\":len(str(p_blocks[i].text))}\n",
    "                                  ,ignore_index=True\n",
    "                                  ,sort=False)\n",
    "    if len(p_blocks_df)>0:\n",
    "        p_blocks_df_groupby_parent_hierarchy=p_blocks_df.groupby(by=['parent_hierarchy'])\n",
    "        p_blocks_df_groupby_parent_hierarchy_sum=p_blocks_df_groupby_parent_hierarchy[['element_text_Count']].sum()            \n",
    "        p_blocks_df_groupby_parent_hierarchy_sum.reset_index(inplace=True)\n",
    "    maxid=p_blocks_df_groupby_parent_hierarchy_sum.loc[p_blocks_df_groupby_parent_hierarchy_sum['element_text_Count'].idxmax()\n",
    "                                                     ,'parent_hierarchy']\n",
    "    merge_text='\\n'.join(p_blocks_df.loc[p_blocks_df['parent_hierarchy']==maxid,'element_text'].to_list())\n",
    "    return merge_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrame(url, label):\n",
    "    parseurl = urlopen(url)\n",
    "    xml_page = parseurl.read()\n",
    "    parseurl.close()\n",
    "\n",
    "    page = BeautifulSoup(xml_page, \"xml\")\n",
    "    news_list = page.findAll(\"item\")\n",
    "    \n",
    "    colNames = []\n",
    "    colNames.append(\"title\")\n",
    "    colNames.append(\"data\")\n",
    "    colNames.append(\"link\")\n",
    "    colNames.append(\"date\")\n",
    "    colNames.append(\"label\")\n",
    "\n",
    "    df = pd.DataFrame(columns=colNames)\n",
    "\n",
    "    titles = []\n",
    "    data = []\n",
    "    links = []\n",
    "    dates = []\n",
    "    labels = []\n",
    "\n",
    "    for getfeed in news_list:\n",
    "        titles.append(getfeed.title.text)\n",
    "        data.append(getArticleText(getfeed.link.text))\n",
    "        links.append(getfeed.link.text)\n",
    "        dates.append(getfeed.pubDate.text)\n",
    "        labels.append(label)\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(titles, data, links, dates, labels)), columns=[\"title\", \"data\", \"link\", \"date\", \"label\"])\n",
    "    #for e in df:\n",
    "     #   if (len(df.loc[e].title) + len(df.loc[e].data) < 15):\n",
    "      #      df.drop(e)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBigrams(bigramCorpus, corpusName):\n",
    "    for word1, word2 in bigrams(corpusName):\n",
    "        bigramCorpus[word1][word2] += 1\n",
    "    for word1 in bigramCorpus:\n",
    "        wordCount = float(sum(bigramCorpus[word1].values()))\n",
    "        for word2 in bigramCorpus[word1]:\n",
    "            bigramCorpus[word1][word2] /= wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrigrams(trigramCorpus, corpusName):\n",
    "    for word1, word2, word3 in trigrams(corpusName):\n",
    "        trigramCorpus[(word1,word2)][word3] += 1\n",
    "    for word1_word2 in trigramCorpus:\n",
    "        wordCount = float(sum(trigramCorpus[word1_word2].values()))\n",
    "        for word3 in trigramCorpus[word1_word2]:\n",
    "            trigramCorpus[word1_word2][word3] /= wordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(df, index):\n",
    "    corpusList = [newsCorpus, busCorpus, healthCorpus, edCorpus, sciCorpus, entCorpus]\n",
    "    bigramList = [bigramNews, bigramBus, bigramHealth, bigramEd, bigramSci, bigramEnt]\n",
    "    trigramList = [trigramNews, trigramBus, trigramHealth, trigramEd, trigramSci, trigramEnt]\n",
    "    labelNames = [\"news\", \"business\", \"health\", \"education\", \"science/environment\", \"entertainment/arts\"]\n",
    "    similarity = [0, 0, 0, 0, 0, 0]\n",
    "    title = df.loc[index].title\n",
    "    title = title.lower()\n",
    "    tokenTitle = title.split(\" \")\n",
    "    data = df.loc[index][\"data\"]\n",
    "    data = data.lower()\n",
    "    tokenData = data.split(\" \")\n",
    "    titleBigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    titleTrigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    dataBigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    dataTrigrams = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for word1, word2 in bigrams(tokenTitle):\n",
    "        titleBigrams[word1][word2] += 1\n",
    "    for word1, word2 in bigrams(tokenData):\n",
    "        dataBigrams[word1][word2] +=1\n",
    "    for word1, word2, word3 in trigrams(tokenTitle):\n",
    "        titleTrigrams[(word2, word2)][word3] += 1\n",
    "    for word1, word2, word3 in trigrams(tokenData):\n",
    "        dataTrigrams[(word1, word2)][word3] += 1\n",
    "    for i in range(len(similarity)):\n",
    "        count = 0;\n",
    "        if labelNames[i] in tokenTitle or labelNames[i] in tokenData:\n",
    "            count = count + 2\n",
    "        \n",
    "        #compare individual words\n",
    "        for w in tokenTitle:\n",
    "            if w in corpusList[i]:\n",
    "                count = count + 1\n",
    "        for w in tokenData:\n",
    "            if w in corpusList[i]:\n",
    "                count = count + 1        \n",
    "                \n",
    "        #compare bigrams\n",
    "        for k in titleBigrams.keys():\n",
    "            if k in bigramList[i].keys():\n",
    "                for secWord in titleBigrams[k].keys():\n",
    "                    if secWord in bigramList[i][k].keys():\n",
    "                        count = count + 2\n",
    "        \n",
    "        for k in dataBigrams.keys():\n",
    "            if k in bigramList[i].keys():\n",
    "                for secWord in dataBigrams[k].keys():\n",
    "                    if secWord in bigramList[i][k].keys():\n",
    "                        count = count + 2\n",
    "                        \n",
    "        #compare trigrams\n",
    "        for k in titleTrigrams.keys():\n",
    "            if k in trigramList[i].keys():\n",
    "                for thirdWord in titleTrigrams[k].keys():\n",
    "                    if thirdWord in trigramList[i][k].keys():\n",
    "                        count = count + 3\n",
    "        for k in dataTrigrams.keys():\n",
    "            if k in trigramList[i].keys():\n",
    "                for thirdWord in dataTrigrams[k].keys():\n",
    "                    if thirdWord in trigramList[i][k].keys():\n",
    "                        count = count + 3\n",
    "        \n",
    "        similarity[i] = count\n",
    "    ind = similarity.index(max(similarity))\n",
    "    return labelNames[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://feeds.bbci.co.uk/news/world/rss.xml\"\n",
    "label1 = \"news\"\n",
    "df = createDataFrame(url, label1)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/uk/rss.xml\"\n",
    "df2 = createDataFrame(url, label1)\n",
    "df = df.append(df2)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/business/rss.xml\"\n",
    "label3 = \"business\"\n",
    "df3 = createDataFrame(url, label3)\n",
    "df = df.append(df3)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/politics/rss.xml\"\n",
    "df4 = createDataFrame(url, label1)\n",
    "df = df.append(df4)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/health/rss.xml\"\n",
    "label5 = \"health\"\n",
    "df5 = createDataFrame(url, label5)\n",
    "df = df.append(df5)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/education/rss.xml\"\n",
    "label6 = \"education\"\n",
    "df6 = createDataFrame(url, label6)\n",
    "df = df.append(df6)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\"\n",
    "label7 = \"science/environment\"\n",
    "df7 = createDataFrame(url, label7)\n",
    "df = df.append(df7)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/technology/rss.xml\"\n",
    "df8 = createDataFrame(url, label7)\n",
    "df = df.append(df8)\n",
    "\n",
    "url = \"http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\"\n",
    "label9 = \"entertainment/arts\"\n",
    "df9 = createDataFrame(url, label9)\n",
    "df = df.append(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanList = []\n",
    "for s in df[\"title\"]:\n",
    "    words = s.split(\" \")\n",
    "    cleanSentence = []\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        if w not in stop_words and w not in punctuation:\n",
    "            if w.endswith(\"ies\"):\n",
    "                w = w[:-3]\n",
    "                w = w + \"y\"\n",
    "            cleanSentence.append(w)\n",
    "    cleanSentence = \" \".join(cleanSentence)\n",
    "    cleanList.append(cleanSentence)\n",
    "df[\"title\"] = cleanList\n",
    "\n",
    "cleanList = []\n",
    "for s in df[\"data\"]:\n",
    "    words = s.split(\" \")\n",
    "    cleanSentence = []\n",
    "    for w in words:\n",
    "        w = w.lower()\n",
    "        if w not in stop_words and w not in punctuation:\n",
    "            if w.endswith(\"ies\"):\n",
    "                w = w[:-3]\n",
    "                w = w + \"y\"\n",
    "            cleanSentence.append(w)\n",
    "    cleanSentence = \" \".join(cleanSentence)\n",
    "    cleanList.append(cleanSentence)\n",
    "df[\"data\"] = cleanList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkpap\\Anaconda2\\envs\\py36\\lib\\site-packages\\wikipedia\\wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\kkpap\\Anaconda2\\envs\\py36\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "bigramNews = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramNews = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "newsCorpus = []\n",
    "createCorpus(newsCorpus, \"news\")\n",
    "createBigrams(bigramNews, newsCorpus)\n",
    "createTrigrams(trigramNews, newsCorpus)\n",
    "\n",
    "bigramBus = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramBus = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "busCorpus = []\n",
    "createCorpus(busCorpus, \"business\")\n",
    "createBigrams(bigramBus, busCorpus)\n",
    "createTrigrams(trigramBus, busCorpus)\n",
    "\n",
    "bigramHealth = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramHealth = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "healthCorpus = []\n",
    "createCorpus(healthCorpus, \"health\")\n",
    "createBigrams(bigramHealth, healthCorpus)\n",
    "createTrigrams(trigramHealth, healthCorpus)\n",
    "\n",
    "bigramEd = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramEd = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "edCorpus = []\n",
    "createCorpus(edCorpus, \"education\")\n",
    "createBigrams(bigramEd, edCorpus)\n",
    "createTrigrams(trigramEd, edCorpus)\n",
    "\n",
    "bigramSci = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramSci = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "sciCorpus = []\n",
    "createCorpus(sciCorpus, \"science\")\n",
    "createCorpus(sciCorpus, \"environment\")\n",
    "createBigrams(bigramSci, sciCorpus)\n",
    "createTrigrams(trigramSci, sciCorpus)\n",
    "\n",
    "bigramEnt = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "trigramEnt = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "entCorpus = []\n",
    "createCorpus(entCorpus, \"entertainment\")\n",
    "createCorpus(entCorpus, \"arts\")\n",
    "createBigrams(bigramEnt, entCorpus)\n",
    "createTrigrams(trigramEnt, entCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>delhi factory fire: 40 dead india blaze</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>pensacola attack: gunman 'played mass-shooting...</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>north korea carry 'very important test'</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>thousands join largest hk protest rally months</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bob hawke 'asked daughter keep rape claim secret'</td>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>original fyre festival?</td>\n",
       "      <td>entertainment/arts</td>\n",
       "      <td>entertainment/arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>turner prize: moment four nominees win prestig...</td>\n",
       "      <td>entertainment/arts</td>\n",
       "      <td>entertainment/arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>turner prize town builds bridges art</td>\n",
       "      <td>entertainment/arts</td>\n",
       "      <td>entertainment/arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>c3po actor: 'i left star wars publicity'</td>\n",
       "      <td>entertainment/arts</td>\n",
       "      <td>science/environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>jack savoretti messages cancer girl anna drysdale</td>\n",
       "      <td>entertainment/arts</td>\n",
       "      <td>news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>258 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title              actual  \\\n",
       "0              delhi factory fire: 40 dead india blaze                news   \n",
       "1    pensacola attack: gunman 'played mass-shooting...                news   \n",
       "2              north korea carry 'very important test'                news   \n",
       "3       thousands join largest hk protest rally months                news   \n",
       "4    bob hawke 'asked daughter keep rape claim secret'                news   \n",
       "..                                                 ...                 ...   \n",
       "253                            original fyre festival?  entertainment/arts   \n",
       "254  turner prize: moment four nominees win prestig...  entertainment/arts   \n",
       "255               turner prize town builds bridges art  entertainment/arts   \n",
       "256           c3po actor: 'i left star wars publicity'  entertainment/arts   \n",
       "257  jack savoretti messages cancer girl anna drysdale  entertainment/arts   \n",
       "\n",
       "               predicted  \n",
       "0                   news  \n",
       "1                   news  \n",
       "2                   news  \n",
       "3                   news  \n",
       "4                   news  \n",
       "..                   ...  \n",
       "253   entertainment/arts  \n",
       "254   entertainment/arts  \n",
       "255   entertainment/arts  \n",
       "256  science/environment  \n",
       "257                 news  \n",
       "\n",
       "[258 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = list(range(len(df.index)))\n",
    "df.index = index\n",
    "\n",
    "table = pd.DataFrame(columns = [\"title\", \"actual\", \"predicted\"])\n",
    "\n",
    "tList = []\n",
    "aList = []\n",
    "pList = []\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i in range(len(df.index)):\n",
    "    tList.append(df.loc[i].title)\n",
    "    aList.append(df.loc[i].label)\n",
    "    comp = compare(df, i)\n",
    "    pList.append(comp)\n",
    "table.title = tList\n",
    "table.actual = aList\n",
    "table.predicted = pList\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.46511627906976744\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(table.index)):\n",
    "    if(table.loc[i].actual == table.loc[i].predicted):\n",
    "        correct = correct + 1\n",
    "\n",
    "print(\"Accuracy: \", correct/len(table.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "artist eats $120,000 banana artwork\tnews\tentertainment/arts\n",
      "mike horn boerge ousland: north pole explorers complete epic trek\tnews\tscience/environment\n",
      "uganda floods: least 16 people dead, red cross says\tnews\tscience/environment\n",
      "match\n",
      "russia doping: athletes wait fear fresh world ban\tnews\tentertainment/arts\n",
      "match\n",
      "vienna opera house stages first opera woman\tnews\tentertainment/arts\n",
      "panipat: bollywood battle 18th century war\tnews\tentertainment/arts\n",
      "match\n",
      "match\n",
      "seychelles: island nation novel way tackle climate change\tnews\tscience/environment\n",
      "match\n",
      "match\n",
      "indian ocean dipole: linked floods bushfires?\tnews\tscience/environment\n",
      "hong kong pro-democracy rally: 'the streets full again'\tnews\tentertainment/arts\n",
      "match\n",
      "cop25 climate change conference: giving environment?\tnews\teducation\n",
      "burkina faso crisis: 'soldiers killed seven members family'\tnews\tbusiness\n",
      "yinka ilori: nigerian narrative art design london\tnews\tentertainment/arts\n",
      "berea college: us university cracked student debt?\tnews\tentertainment/arts\n",
      "life death: seven kids came back dead\tnews\tscience/environment\n",
      "kenya pollution: air sensors helping people fight pollution\tnews\thealth\n",
      "match\n",
      "match\n",
      "general election 2019: party final campaign push poll nears\tnews\thealth\n",
      "match\n",
      "manchester derby racist abuse claim: man arrested\tnews\tentertainment/arts\n",
      "rushden stabbing: boy, 13, man arrested woman's death\tnews\tentertainment/arts\n",
      "match\n",
      "six arrests stabbings outside southport hotel\tnews\thealth\n",
      "match\n",
      "match\n",
      "'swanfall' slimbridge marks start winter\tnews\tscience/environment\n",
      "election 2019: checks great britain ni?\tnews\tscience/environment\n",
      "quiz: test election 2019 knowledge 14 questions\tnews\tentertainment/arts\n",
      "general election 2019: questions climate change environment\tnews\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "tiger came tea: judith kerr story heads tv\tnews\tscience/environment\n",
      "match\n",
      "southend photographer hopes essex girls project 'challenge stereotype'\tnews\tentertainment/arts\n",
      "sustainable christmas\tnews\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "firing arrows deafblind\tnews\tbusiness\n",
      "match\n",
      "match\n",
      "match\n",
      "storm atiyah: 'status red' wind warning county kerry\tnews\thealth\n",
      "drug crime: cardiff arrests 'making difference'\tnews\thealth\n",
      "river calder searched missing 81-year-old dewsbury man\tnews\tscience/environment\n",
      "general election 2019: johnson insists ni-gb goods checks brexit\tbusiness\tnews\n",
      "china exports fall us trade war continues\tbusiness\tnews\n",
      "hmv faces music christmas crunch test\tbusiness\tentertainment/arts\n",
      "avanti starts running west coast main line virgin franchise ends\tbusiness\tnews\n",
      "trade disputes settlement system facing crisis\tbusiness\tnews\n",
      "seychelles: island nation novel way tackle climate change\tbusiness\tscience/environment\n",
      "pensioner £193,000 inheritance battle sort code error\tbusiness\tnews\n",
      "thomas cook: 'they told money safe'\tbusiness\tnews\n",
      "virgin trains: final service departs uk's longest-running rail franchise ends\tbusiness\tnews\n",
      "sweden's ericsson pay $1bn settle us corruption probe\tbusiness\tnews\n",
      "general election 2019: labour pledges electrify england's bus fleet\tbusiness\tnews\n",
      "eddie stobart saved collapse crunch vote\tbusiness\tnews\n",
      "us jobs growth jumps november\tbusiness\thealth\n",
      "elon musk wins defamation case 'pedo guy' tweet caver\tbusiness\tnews\n",
      "lloyd's london staff told behave christmas party\tbusiness\thealth\n",
      "west coast rail: preston-to-scotland rail fares capped\tbusiness\tentertainment/arts\n",
      "macron pension reform: strike continues second day\tbusiness\tnews\n",
      "uber 6,000 us sexual assault reports two years\tbusiness\thealth\n",
      "match\n",
      "garuda airline boss lose job smuggled motorbike\tbusiness\tnews\n",
      "thomas cook customers face refund delays\tbusiness\tscience/environment\n",
      "deepmind co-founder mustafa suleyman switches google\tbusiness\tnews\n",
      "plum: users money app complain long delays withdrawals\tbusiness\tscience/environment\n",
      "bug busters: tech behind new vaccines\tbusiness\tscience/environment\n",
      "macron pension reform: french workers strike?\tbusiness\tnews\n",
      "cambodia's bicycle firms face bumps road\tbusiness\tnews\n",
      "coral dredging: 'it's going cause irreversible damage'\tbusiness\tscience/environment\n",
      "world's financial plumbing pressure?\tbusiness\tscience/environment\n",
      "china-us rivalry dividing internet\tbusiness\tnews\n",
      "'we try make good food emotionally addictive'\tbusiness\tnews\n",
      "'i match clothes pug'\tbusiness\tentertainment/arts\n",
      "make phone battery last longer\tbusiness\tscience/environment\n",
      "old fridges recycled make new ones?\tbusiness\tscience/environment\n",
      "loan sharks cash black friday spending spree\tbusiness\tscience/environment\n",
      "general election 2019: look economy\tbusiness\tnews\n",
      "match\n",
      "egg box became million dollar idea\tbusiness\tnews\n",
      "lotus evija: £2.2m electric hypercar\tbusiness\tnews\n",
      "raiding factories, freeing children\tbusiness\tnews\n",
      "missguided boss black friday: 'if play, die'\tbusiness\tnews\n",
      "sticking plasters became million dollar idea\tbusiness\tnews\n",
      "match\n",
      "match\n",
      "general election 2019: party final campaign push poll nears\tnews\thealth\n",
      "match\n",
      "match\n",
      "match\n",
      "quiz: test election 2019 knowledge 14 questions\tnews\tentertainment/arts\n",
      "general election 2019: questions climate change environment\tnews\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "poll tracker: party compare?\tnews\tscience/environment\n",
      "really simple guide election\tnews\thealth\n",
      "match\n",
      "sturgeon: snp would support minority labour government\tnews\teducation\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "question time debate: election leaders fact-checked\tnews\tscience/environment\n",
      "match\n",
      "general election: labour's £6,000 bill claim fact-checked\tnews\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "general election 2019: people still vote according class?\tnews\tscience/environment\n",
      "general election 2019: really simple guide\tnews\thealth\n",
      "match\n",
      "match\n",
      "doddie weir: rugby legend give mnd\thealth\tentertainment/arts\n",
      "'i addicted exercise'\thealth\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "general election 2019: many 'new' nhs hospitals conservatives building?\thealth\tnews\n",
      "shropshire baby deaths: 'this review babies' legacy'\thealth\tscience/environment\n",
      "general election 2019: nhs news wraps polling day\thealth\tnews\n",
      "match\n",
      "match\n",
      "'how go back debbie was?'\thealth\tscience/environment\n",
      "microblading: therapist tattoos eyebrows cancer patients\thealth\tnews\n",
      "'my heart dead six hours'\thealth\tnews\n",
      "eggs protect us flu?\thealth\tnews\n",
      "jack savoretti messages cancer girl anna drysdale\thealth\tnews\n",
      "‘i got hiv first time’\thealth\tnews\n",
      "police probe alleged fraud scottish qualifications authority\teducation\tnews\n",
      "poor children lose exercise, research suggests\teducation\thealth\n",
      "mum gave birth 15 graduates daughter watching\teducation\tnews\n",
      "newcastle university stalker's return prompts petition\teducation\tnews\n",
      "match\n",
      "general election 2019: labour pledges cap class sizes 30 pupils\teducation\tscience/environment\n",
      "match\n",
      "oxford university launches scholarship scheme black britons\teducation\tentertainment/arts\n",
      "'integrate survive' says primary schools report\teducation\tscience/environment\n",
      "jewish schools 'pressurise parents take children sex ed lessons'\teducation\tscience/environment\n",
      "smartphone 'addiction': young people 'panicky' denied mobiles\teducation\tscience/environment\n",
      "conservatives pledge boost ofsted rather scrap\teducation\thealth\n",
      "match\n",
      "match\n",
      "match\n",
      "bad dreams 'help control fear awake'\teducation\tscience/environment\n",
      "'half women carers age 46'\teducation\thealth\n",
      "university lecturers' strikes: students affects\teducation\thealth\n",
      "foetal alcohol spectrum disorder: mum's fight diagnosis\teducation\thealth\n",
      "general election 2019: wolverhampton carer's view\teducation\tnews\n",
      "match\n",
      "match\n",
      "teen engineer: 'let introduce laboratory'\teducation\tnews\n",
      "bullying: teachers share story video pupils\teducation\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "sydney zoo: world still need big zoos?\tscience/environment\tentertainment/arts\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "greta united nations climate talks one year apart\tscience/environment\tnews\n",
      "cop25 climate change conference: giving environment?\tscience/environment\teducation\n",
      "match\n",
      "drones help counting seals\tscience/environment\tnews\n",
      "match\n",
      "elon musk wins defamation case 'pedo guy' tweet caver\tscience/environment\tnews\n",
      "sweden's ericsson pay $1bn settle us corruption probe\tscience/environment\tnews\n",
      "match\n",
      "nhs e-health systems 'risk patient safety'\tscience/environment\thealth\n",
      "general election 2019: labour pledges electrify england's bus fleet\tscience/environment\tnews\n",
      "general election 2019: reddit says uk-us trade talks document leak 'linked russia'\tscience/environment\tnews\n",
      "heavy fine chinese firm unlicensed game\tscience/environment\tentertainment/arts\n",
      "match\n",
      "uber 6,000 us sexual assault reports two years\tscience/environment\thealth\n",
      "chinese residents worry rise facial recognition\tscience/environment\tnews\n",
      "deepmind co-founder mustafa suleyman switches google\tscience/environment\tnews\n",
      "apple iphone 11 pro 'can override location settings'\tscience/environment\tnews\n",
      "rory cellan-jones: reporting news parkinson's\tscience/environment\tnews\n",
      "electric eel lights christmas tree news\tscience/environment\tnews\n",
      "lotus evija: £2.2m electric hypercar\tscience/environment\tnews\n",
      "facebook's zuckerberg rare interview inside home\tscience/environment\tnews\n",
      "jet suit man fly royal navy ship news\tscience/environment\tnews\n",
      "match\n",
      "match\n",
      "china-us rivalry dividing internet\tscience/environment\tnews\n",
      "amazon: 'peak season' mean employees?\tscience/environment\tnews\n",
      "match\n",
      "goo hara trauma south korea's spy cam victims\tscience/environment\tentertainment/arts\n",
      "match\n",
      "match\n",
      "tiger came tea: judith kerr story heads tv\tentertainment/arts\tscience/environment\n",
      "match\n",
      "morpurgo's christmas (fire) cracker\tentertainment/arts\tnews\n",
      "match\n",
      "star wars: leicestershire factory centre toy galaxy\tentertainment/arts\tnews\n",
      "girl, 15, charged singer katherine jenkins mugged\tentertainment/arts\tnews\n",
      "match\n",
      "match\n",
      "match\n",
      "match\n",
      "chess became theatre event\tentertainment/arts\tscience/environment\n",
      "match\n",
      "match\n",
      "match\n",
      "grand tour: jeremy clarkson show confronts climate change\tentertainment/arts\tscience/environment\n",
      "hollywood film based 1875 lan mining disaster planned\tentertainment/arts\tnews\n",
      "match\n",
      "match\n",
      "match\n",
      "c3po actor: 'i left star wars publicity'\tentertainment/arts\tscience/environment\n",
      "jack savoretti messages cancer girl anna drysdale\tentertainment/arts\tnews\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(table.index)):\n",
    "    if table.loc[i].actual != table.loc[i].predicted:\n",
    "        print(f\"{table.loc[i].title}\\t{table.loc[i].actual}\\t{table.loc[i].predicted}\")\n",
    "    else:\n",
    "        print(\"match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
